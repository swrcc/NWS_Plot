{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff59d0a-6318-4109-a377-f22486c1d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_972396/2669566808.py:4: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing files in the directory: ny_warnings_data\n",
      "Data successfully downloaded and saved as ny_warnings_data.zip\n",
      "Shapefile found: wwa_202405010000_202407240000.shp\n",
      "All necessary shapefile components are present.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "# Define the base URL for the IEM data request\n",
    "base_url = 'https://mesonet.agron.iastate.edu/cgi-bin/request/gis/watchwarn.py'\n",
    "\n",
    "# Define the start and end timestamps\n",
    "start_date = '2024-05-01T00:00Z'\n",
    "# end_date = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "end_date = '2024-07-24T00:00Z'\n",
    "\n",
    "# Define the parameters\n",
    "params = {\n",
    "    'accept': 'shapefile',         # Request data in shapefile format\n",
    "    'sts': start_date,             # Start timestamp\n",
    "    'ets': end_date,               # End timestamp\n",
    "    'location_group': 'states',    # Data grouped by state\n",
    "    'states': 'NY'                 # Data for New York\n",
    "}\n",
    "\n",
    "# Directory to store shapefiles\n",
    "shapefile_dir = 'ny_warnings_data'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(shapefile_dir):\n",
    "    # Remove all files in the directory\n",
    "    for file_name in os.listdir(shapefile_dir):\n",
    "        file_path = os.path.join(shapefile_dir, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(f'Cleared existing files in the directory: {shapefile_dir}')\n",
    "else:\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(shapefile_dir)\n",
    "    print(f'Created directory: {shapefile_dir}')\n",
    "\n",
    "# Download the shapefile zip file\n",
    "response = requests.get(base_url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the response content to a file\n",
    "    zip_filename = 'ny_warnings_data.zip'\n",
    "    with open(zip_filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Data successfully downloaded and saved as {zip_filename}')\n",
    "    \n",
    "    # Extract the downloaded zip file\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        # Extract all files to the specified directory\n",
    "        zip_ref.extractall(shapefile_dir)\n",
    "        \n",
    "        # List all files in the extraction directory\n",
    "        extracted_files = os.listdir(shapefile_dir)\n",
    "        \n",
    "        # Find the shapefile in the extracted files\n",
    "        shapefile_name = None\n",
    "        for file in extracted_files:\n",
    "            if file.endswith('.shp'):\n",
    "                shapefile_name = file\n",
    "                break\n",
    "        \n",
    "        if shapefile_name:\n",
    "            print(f'Shapefile found: {shapefile_name}')\n",
    "        else:\n",
    "            print('No shapefile found in the extracted files.')\n",
    "        \n",
    "        # Check if all necessary shapefile components are present\n",
    "        shapefile_components = ['.shp', '.shx', '.dbf', '.prj']\n",
    "        missing_components = [ext for ext in shapefile_components if not any(f.endswith(ext) for f in extracted_files)]\n",
    "        \n",
    "        if not missing_components:\n",
    "            print('All necessary shapefile components are present.')\n",
    "        else:\n",
    "            print(f'Missing shapefile components: {missing_components}')\n",
    "else:\n",
    "    print(f'Failed to retrieve data. Status code: {response.status_code}')\n",
    "\n",
    "# Define paths to shapefiles\n",
    "if shapefile_name:\n",
    "    nws_shapefile_path = os.path.join(shapefile_dir, shapefile_name)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Shapefile not found in the extracted files.\")\n",
    "\n",
    "state_shapefile_path = '/nfs/home11/ugrad/2020/tr588861/SWRCC/State_Shapefiles/state/State.shp'\n",
    "county_shapefile_path = '/nfs/home11/ugrad/2020/tr588861/SWRCC/State_Shapefiles/state/Counties.shp'\n",
    "\n",
    "# Load the NWS data\n",
    "nws_data = gpd.read_file(nws_shapefile_path)\n",
    "state_boundary = gpd.read_file(state_shapefile_path)\n",
    "county_boundaries = gpd.read_file(county_shapefile_path)\n",
    "\n",
    "# Load the VTEC dictionaries from CSV files\n",
    "phenomena_df = pd.read_csv('Decoder/VTEC_PHENOMENA.csv', header=None, names=['Code', 'Description'])\n",
    "significance_df = pd.read_csv('Decoder/VTEC_SIGNIFICANCE.csv', header=None, names=['Code', 'Description'])\n",
    "colors_df = pd.read_csv('Decoder/NWS_COLORS.csv', header=None, names=['Code', 'Color'])\n",
    "\n",
    "# Convert CSV data to dictionaries\n",
    "VTEC_PHENOMENA = dict(zip(phenomena_df['Code'], phenomena_df['Description']))\n",
    "VTEC_SIGNIFICANCE = dict(zip(significance_df['Code'], significance_df['Description']))\n",
    "NWS_COLORS = dict(zip(colors_df['Code'], colors_df['Color']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b5d8f4-1f65-4a0d-94cd-e8e341a35098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert UTC to Eastern Time\n",
    "def convert_to_eastern(utc_time):\n",
    "    from pytz import timezone\n",
    "    eastern = timezone('US/Eastern')\n",
    "    utc = timezone('UTC')\n",
    "    utc_time = pd.to_datetime(utc_time).tz_localize('UTC')\n",
    "    eastern_time = utc_time.astimezone(eastern)\n",
    "    return eastern_time\n",
    "\n",
    "# Convert time columns to Eastern Time\n",
    "nws_data['ISSUED_ET'] = nws_data['ISSUED'].apply(convert_to_eastern)\n",
    "nws_data['EXPIRED_ET'] = nws_data['EXPIRED'].apply(convert_to_eastern)\n",
    "\n",
    "# Filter the data for new warnings\n",
    "new_warnings = nws_data[(nws_data['STATUS'] == 'NEW') & (nws_data['GTYPE'] == 'P')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0fc9c8-7106-4adf-9f01-164a4028f587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maps saved successfully in the Output_Maps directory.\n"
     ]
    }
   ],
   "source": [
    "# Function to create a Folium map for a specific warning type and date\n",
    "def create_warning_map(warning_type, center=[43.0, -75.0], zoom=7, date=None):\n",
    "    if date:\n",
    "        warning_data = new_warnings[(new_warnings['PHENOM'] == warning_type) & (new_warnings['ISSUED_ET'].dt.date == date)]\n",
    "    else:\n",
    "        warning_data = new_warnings[new_warnings['PHENOM'] == warning_type]\n",
    "        \n",
    "    m = folium.Map(location=center, zoom_start=zoom)\n",
    "    \n",
    "    # Add state boundary to the map\n",
    "    folium.GeoJson(\n",
    "        state_boundary,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'none',\n",
    "            'color': 'black',\n",
    "            'weight': 3\n",
    "        }\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add county boundaries to the map\n",
    "    folium.GeoJson(\n",
    "        county_boundaries,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'none',\n",
    "            'color': 'gray',\n",
    "            'weight': 0.5\n",
    "        }\n",
    "    ).add_to(m)\n",
    "    \n",
    "    for _, row in warning_data.iterrows():\n",
    "        # Decode the PHENOM and SIG codes\n",
    "        phenom_code = row['PHENOM']\n",
    "        sig_code = row['SIG']\n",
    "        phenom_desc = VTEC_PHENOMENA.get(phenom_code, phenom_code)\n",
    "        sig_desc = VTEC_SIGNIFICANCE.get(sig_code, sig_code)\n",
    "        \n",
    "        # Get the color for the warning\n",
    "        color_key = f\"{phenom_code}.{sig_code}\"\n",
    "        color = NWS_COLORS.get(color_key, '#000000')  # Default to black if not found\n",
    "        \n",
    "        # Extract the polygon geometry\n",
    "        geom = row['geometry']\n",
    "        \n",
    "        # Convert Shapely geometry to GeoJSON\n",
    "        geo_json = gpd.GeoSeries([geom]).__geo_interface__\n",
    "        \n",
    "        # Add the polygon to the map\n",
    "        folium.GeoJson(\n",
    "            geo_json,\n",
    "            style_function=lambda x, color=color: {\n",
    "                'fillColor': color,\n",
    "                'color': color,\n",
    "                'weight': 2,\n",
    "                'fillOpacity': 0.4\n",
    "            },\n",
    "            tooltip=f\"{phenom_desc} {sig_desc}\"\n",
    "        ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def create_combined_map(center=[43.0, -75.0], zoom=7):\n",
    "    combined_map = folium.Map(location=center, zoom_start=zoom)\n",
    "    \n",
    "    # Add state boundary to the map\n",
    "    folium.GeoJson(\n",
    "        state_boundary,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'none',\n",
    "            'color': 'black',\n",
    "            'weight': 3\n",
    "        }\n",
    "    ).add_to(combined_map)\n",
    "    \n",
    "    # Add county boundaries to the map\n",
    "    folium.GeoJson(\n",
    "        county_boundaries,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'none',\n",
    "            'color': 'gray',\n",
    "            'weight': 0.5\n",
    "        }\n",
    "    ).add_to(combined_map)\n",
    "    \n",
    "    for warning_type in ['SV', 'TO', 'FF']:\n",
    "        warning_data = new_warnings[new_warnings['PHENOM'] == warning_type]\n",
    "        \n",
    "        for _, row in warning_data.iterrows():\n",
    "            # Decode the PHENOM and SIG codes\n",
    "            phenom_code = row['PHENOM']\n",
    "            sig_code = row['SIG']\n",
    "            phenom_desc = VTEC_PHENOMENA.get(phenom_code, phenom_code)\n",
    "            sig_desc = VTEC_SIGNIFICANCE.get(sig_code, sig_code)\n",
    "            \n",
    "            # Get the color for the warning\n",
    "            color_key = f\"{phenom_code}.{sig_code}\"\n",
    "            color = NWS_COLORS.get(color_key, '#000000')  # Default to black if not found\n",
    "            \n",
    "            # Extract the polygon geometry\n",
    "            geom = row['geometry']\n",
    "            \n",
    "            # Convert Shapely geometry to GeoJSON\n",
    "            geo_json = gpd.GeoSeries([geom]).__geo_interface__\n",
    "            \n",
    "            # Add the polygon to the map\n",
    "            folium.GeoJson(\n",
    "                geo_json,\n",
    "                style_function=lambda x, color=color: {\n",
    "                    'fillColor': color,\n",
    "                    'color': color,\n",
    "                    'weight': 2,\n",
    "                    'fillOpacity': 0.4\n",
    "                },\n",
    "                tooltip=f\"{phenom_desc} {sig_desc}\"\n",
    "            ).add_to(combined_map)\n",
    "    \n",
    "    return combined_map\n",
    "\n",
    "# Create maps for each warning type\n",
    "severe_thunderstorm_map = create_warning_map('SV')\n",
    "tornado_map = create_warning_map('TO')\n",
    "flash_flood_map = create_warning_map('FF')\n",
    "\n",
    "# Create the Output_Maps directory if it doesn't exist\n",
    "output_dir = 'Output_Maps'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the maps to the Output_Maps directory\n",
    "severe_thunderstorm_map.save(os.path.join(output_dir, 'severe_thunderstorm_map.html'))\n",
    "tornado_map.save(os.path.join(output_dir, 'tornado_map.html'))\n",
    "flash_flood_map.save(os.path.join(output_dir, 'flash_flood_map.html'))\n",
    "\n",
    "# Create the combined map with all dates\n",
    "combined_map = create_combined_map()\n",
    "\n",
    "# Save the combined map\n",
    "combined_map_filename = os.path.join('Output_Maps', 'combined_map.html')\n",
    "combined_map.save(combined_map_filename)\n",
    "\n",
    "print('Maps saved successfully in the Output_Maps directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479c6c54-0613-4cb8-b60e-004af0f9b5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary and maps have been generated and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_972396/2705027570.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_data['PHENOM'] = unique_data['PHENOM'].map(VTEC_PHENOMENA)\n",
      "/tmp/ipykernel_972396/2705027570.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_data['SIG'] = unique_data['SIG'].map(VTEC_SIGNIFICANCE)\n",
      "/tmp/ipykernel_972396/2705027570.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_data['ALERT_TYPE'] = unique_data['PHENOM'] + ' ' + unique_data['SIG']\n"
     ]
    }
   ],
   "source": [
    "# Reproject to UTM zone 18N (EPSG:32618)\n",
    "nws_data = nws_data.to_crs(epsg=32618)\n",
    "state_boundary = state_boundary.to_crs(epsg=32618)\n",
    "\n",
    "# Spatial join to find the intersection\n",
    "intersection = gpd.overlay(nws_data, state_boundary, how='intersection')\n",
    "\n",
    "# Filter by minimum area within NY (adjust threshold as needed)\n",
    "min_area_threshold = 1e6  # 1,000,000 square meters (1 square kilometer)\n",
    "intersection['area'] = intersection.geometry.area\n",
    "filtered_intersection = intersection[intersection['area'] >= min_area_threshold]\n",
    "\n",
    "# Convert to DataFrame for further processing\n",
    "filtered_data = pd.DataFrame(filtered_intersection.drop(columns='geometry'))\n",
    "\n",
    "# Convert UTC to Eastern Time\n",
    "filtered_data['ISSUED'] = pd.to_datetime(filtered_data['ISSUED'], format='%Y%m%d%H%M')\n",
    "filtered_data['ISSUED_ET'] = filtered_data['ISSUED'].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "\n",
    "# Extract the date part for easier filtering\n",
    "filtered_data['ISSUED_DATE_ET'] = filtered_data['ISSUED_ET'].dt.date\n",
    "\n",
    "# Remove duplicates to keep only the 'NEW' status and 'P' type\n",
    "unique_data = filtered_data[(filtered_data['STATUS'] == 'NEW') & (filtered_data['GTYPE'] == 'P')]\n",
    "\n",
    "# Translate PHENOM and SIG columns\n",
    "unique_data['PHENOM'] = unique_data['PHENOM'].map(VTEC_PHENOMENA)\n",
    "unique_data['SIG'] = unique_data['SIG'].map(VTEC_SIGNIFICANCE)\n",
    "\n",
    "# Create a new column for the combined alert type\n",
    "unique_data['ALERT_TYPE'] = unique_data['PHENOM'] + ' ' + unique_data['SIG']\n",
    "\n",
    "# Deduplicate based on the maximum area for each alert type and time\n",
    "max_area_alerts = unique_data.loc[\n",
    "    unique_data.groupby(['ISSUED_ET', 'ALERT_TYPE'])['area'].idxmax()\n",
    "]\n",
    "\n",
    "# Group by date and alert type and count occurrences\n",
    "summary = max_area_alerts.groupby(['ISSUED_DATE_ET', 'ALERT_TYPE']).size().reset_index(name='Count')\n",
    "\n",
    "# Save the summary to a CSV file in the Output_Maps directory\n",
    "summary_csv_path = os.path.join(output_dir, 'nws_warning_summary.csv')\n",
    "summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(\"Summary and maps have been generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f2bdb-90ea-48b7-9051-580c4151a70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 August 2023 Environment",
   "language": "python",
   "name": "aug23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
